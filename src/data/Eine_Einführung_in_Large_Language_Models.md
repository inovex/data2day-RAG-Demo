# Eine Einführung in Large Language Models
ChatGPT ist fast schon zum Synonym für große Sprachmodelle geworden. Doch das
ist nur die Spitze des Eisbergs – es gibt diese Modelle in viel mehr
unterschiedlichen Ausprägungen.  
  
Nach der Einführung der Transformer-Architektur durch Google gab es eine wahre
Explosion an neuen Modellen. Grundsätzlich kann man zwischen Encoder- und
(generativen) Decoder-Modellen unterscheiden, die sich für unterschiedliche
Aufgaben eignen.  
  
Dieser Vortrag gibt einen Überblick über die Architektur und die Entwicklung
der unterschiedlichen Modelle. Er zeigt, dass sich Modelle auch leicht auf
eigener Hardware oder mit kostenlosen Cloud-Diensten ausprobieren lassen und
wie man das mit etwas Mühe auch ohne GPU auf der eigenen Hardware ausprobieren
kann.  
  
Der Ausblick wagt eine Vorhersage, wie es mit den großen Sprachmodellen
vielleicht weitergehen könnte.
## Vorkenntnisse
Mit Sprachmodellen als Anwender hat vermutlich jeder/r schon gearbeitet.
Grundkenntnisse in Machine Learning sind ebenso hilfreich wie ein
Grundverständnis von CPUs, GPUs und Arbeitsspeicher.
## Lernziele
  * Verständnis der Architektur von großen Sprachmodellen  
  * Unterscheidung von Encoder- und Decoder-Modellen  
  * Optimierungsmöglichkeiten für Sprachmodelle  
  * Kenntnis unterschiedlicher generativer Modelle  
  * "Gefühl" für die weitere Entwicklung
## Speaker
![Christian Winkler](/common/images/numbers/22580_1.jpg)  
**Christian Winkler** beschäftigt sich seit vielen Jahre mit künstlicher
Intelligenz, speziell in der automatisierten Analyse natürlichsprachiger Texte
(NLP). Als Professor an der TH Nürnberg konzentriert er sich auf die
Optimierung von User Experience mithilfe moderner Verfahren. Er forscht und
publiziert zu Natural Language Processing und ist regelmäßig Sprecher auf
Machine Learning-Konferenzen.
[Jetzt Tickets sichern](https://data2day.de/tickets.php)